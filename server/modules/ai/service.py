from __future__ import annotations
from fastapi import Request
from typing import List, Dict, Any
from pathlib import Path
from textwrap import dedent
from joblib import load
import numpy as np

from server.modules.ai.schemas import MultipleNewsInput, ClassificationMultipleNewsOutput, ClassificationNewOutput, NewsAnalysisResponse, NewsInput
from server.config import MODEL_PATH
import text_hammer as th


# ===================== Preprocessing =====================
def text_preprocessing(text: str) -> str:
    text = (text or "").lower()
    text = th.cont_exp(text)                # don't -> do not
    text = th.remove_rt(text)               # remove "rt"
    text = th.remove_emails(text)           # user@example.com
    text = th.remove_urls(text)             # http, https
    text = th.remove_html_tags(text)        # <p>...</p>
    text = th.remove_stopwords(text)        # i, me, my, ...
    text = th.remove_accented_chars(text)   # cafÃ© -> cafe
    text = th.remove_special_chars(text)    # #, @, ...
    text = th.make_base(text)               # ran -> run
    return text.strip()


# ===================== Model (cached) =====================
_MODEL = None
_CLASSES = None  # will hold np.ndarray like [0,1,2]

def _get_model():
    global _MODEL, _CLASSES
    if _MODEL is None:
        path = Path(MODEL_PATH)
        if not path.exists():
            raise FileNotFoundError(f"MODEL_PATH not found: {path}")
        _MODEL = load(path)
        if not hasattr(_MODEL, "predict_proba"):
            raise AttributeError("Loaded model does not implement predict_proba")
        if not hasattr(_MODEL, "classes_"):
            raise AttributeError("Loaded model has no attribute classes_")
        _CLASSES = np.array(_MODEL.classes_)  # expect [0,1,2]
    return _MODEL


# ===================== Inference helpers =====================
def predict_sentiment(pipe, text: str, preprocess_fn=None) -> Dict[str, Any]:
    """Tráº£ vá» processed_text, proba theo lá»›p gá»‘c, vÃ  nhÃ£n dá»± Ä‘oÃ¡n."""
    processed_text = preprocess_fn(text) if preprocess_fn else text
    proba = pipe.predict_proba([processed_text])[0]  # shape (n_classes,)
    classes = getattr(pipe, "classes_", range(len(proba)))
    proba_dict = {int(k): float(v) for k, v in zip(classes, proba)}
    label = pipe.predict([processed_text])[0]
    return {
        "processed_text": processed_text,
        "proba": proba_dict,
        "label": label,
    }


def _map_012_to_pos_neg_neu(proba_by_class: Dict[int, float]) -> Dict[str, float]:

    neg = float(proba_by_class.get(0, 0.0))
    neu = float(proba_by_class.get(1, 0.0))
    pos = float(proba_by_class.get(2, 0.0))

    # Ä‘áº£m báº£o tá»•ng ~ 1.0 (trong trÆ°á»ng há»£p model/proba rounding)
    s = neg + neu + pos
    if s > 0:
        neg, neu, pos = neg / s, neu / s, pos / s

    return {"pos": pos, "neg": neg, "neu": neu}


# ===================== Main service =====================
def classify_news(news_data: List[NewsInput]) -> ClassificationMultipleNewsOutput:
    model = _get_model()
    results: List[ClassificationNewOutput] = []

    for news in news_data:
        title = news.title or ""
        description = news.description or ""
        publish_date = news.publish_date

        text = f"{title} {description}".strip()
        pred = predict_sentiment(model, text, preprocess_fn=text_preprocessing)
        mapped = _map_012_to_pos_neg_neu(pred["proba"])

        results.append(
            ClassificationNewOutput(
                title=title,
                description=description,
                publish_date=publish_date,
                **mapped,
            )
        )

    return ClassificationMultipleNewsOutput(news=results)


# server/services/ai_service.py
import os
from typing import List
from fastapi import HTTPException, Request
from dotenv import load_dotenv
from server.modules.ai.schemas import NewsAnalysisResponse, NewsAnalysisInput
import google.generativeai as genai
load_dotenv()



GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL   = os.getenv("GEMINI_MODEL")

SYSTEM_PROMPT_FOR_BULK_ANALYSIS = dedent("""
[ROLE / SYSTEM]  
Báº¡n lÃ  má»™t nhÃ  Ä‘áº§u tÆ° vÃ  chuyÃªn gia phÃ¢n tÃ­ch thá»‹ trÆ°á»ng tÃ i chÃ­nh.  
Nhiá»‡m vá»¥ cá»§a báº¡n: Ä‘á»c danh sÃ¡ch cÃ¡c tin tá»©c (má»—i tin cÃ³ title vÃ  description, cÃ³ thá»ƒ lÃ  tiáº¿ng Anh) vÃ  phÃ¢n loáº¡i gÃ³c nhÃ¬n tÃ¢m lÃ½ cá»§a nhÃ  giao dá»‹ch theo 3 nhÃ³m:  
- Positive (tin tá»©c mang láº¡i ká»³ vá»ng, niá»m tin, Ä‘á»™ng lá»±c Ä‘áº§u tÆ° hoáº·c tÃ¡c Ä‘á»™ng tÃ­ch cá»±c Ä‘áº¿n thá»‹ trÆ°á»ng/tÃ i sáº£n).  
- Neutral (tin tá»©c trung láº­p, chá»‰ cung cáº¥p thÃ´ng tin, chÆ°a Ä‘á»§ Ä‘á»ƒ tÃ¡c Ä‘á»™ng máº¡nh tá»›i tÃ¢m lÃ½ thá»‹ trÆ°á»ng).  
- Negative (tin tá»©c mang láº¡i lo ngáº¡i, rá»§i ro, tÃ¢m lÃ½ bi quan hoáº·c tÃ¡c Ä‘á»™ng tiÃªu cá»±c Ä‘áº¿n thá»‹ trÆ°á»ng/tÃ i sáº£n).  

âš ï¸ QUAN TRá»ŒNG: LuÃ´n dá»‹ch vÃ  viáº¿t pháº§n pháº£n há»“i **báº±ng tiáº¿ng Viá»‡t** dÃ¹ tin tá»©c gá»‘c lÃ  tiáº¿ng Anh.  

[OUTPUT FORMAT]  
Viáº¿t theo dáº¡ng tin nháº¯n, tráº£ lá»i tiáº¿ng Viá»‡t, cÃ³ emoji vÃ  xuá»‘ng dÃ²ng rÃµ rÃ ng, vÃ­ dá»¥: 
---
ðŸ“Š PhÃ¢n tÃ­ch tin tá»©c {ten_ngá»¯_cáº£nh}:  

âœ… **Positive:** 
- [TiÃªu Ä‘á»] - [MÃ´ táº£] - (Tá»« {thá»i_gian_Ä‘Äƒng_bÃ i})
- [TiÃªu Ä‘á»] - [MÃ´ táº£] - (Tá»« {thá»i_gian_Ä‘Äƒng_bÃ i})
- ...                                        
âš–ï¸ **Neutral:** 
- [TiÃªu Ä‘á»] - [MÃ´ táº£] - (Tá»« {thá»i_gian_Ä‘Äƒng_Ä‘áº§u})
- [TiÃªu Ä‘á»] - [MÃ´ táº£] - (Tá»« {thá»i_gian_Ä‘Äƒng_bÃ i})
- ...                                         
âš ï¸ **Negative:** 
- [TiÃªu Ä‘á»] - [MÃ´ táº£] - (Tá»« {thá»i_gian_Ä‘Äƒng_Ä‘áº§u})
- [TiÃªu Ä‘á»] - [MÃ´ táº£] - (Tá»« {thá»i_gian_Ä‘Äƒng_bÃ i})
- ...
ðŸ“Œ **Káº¿t luáº­n:** [Káº¿t luáº­n ngáº¯n gá»n tÃ¢m lÃ½ chung ]
---
[NOTE]  
- CÃ³ thá»ƒ rÃºt gá»n mÃ´ táº£ Ä‘á»ƒ giá»‘ng tin nháº¯n Zalo.  
- Táº¥t cáº£ Ä‘áº§u ra báº¯t buá»™c lÃ  tiáº¿ng Viá»‡t.  
- Pháº§n **Káº¿t luáº­n** pháº£i khÃ¡ch quan, tá»•ng há»£p tá»« cÃ¡c nhÃ³m tin, khÃ´ng thÃªm quan Ä‘iá»ƒm cÃ¡ nhÃ¢n.
""").strip()

def _build_user_prompt(payload: NewsAnalysisInput) -> str:
    lines: List[str] = []
    lines.append("Dá»¯ liá»‡u Ä‘áº§u vÃ o gá»“m nhiá»u bÃ i:")
    for i, item in enumerate(payload.news):
        lines.append(f"\n--- BÃ i #{i} ---")
        lines.append(f"Title: {item.title}")
        lines.append(f"Description: {item.description}")
        # THá»NG NHáº¤T field lÃ  publish_date
        if getattr(item, "publish_date", None):
            lines.append(f"Publish Date: {item.publish_date}")
        lines.append(f"Scores: pos={item.pos:.3f}, neg={item.neg:.3f}, neu={item.neu:.3f}")
    lines.append(
        "\nYÃªu cáº§u: Chá»‰ tráº£ vá» PHÃ‚N TÃCH CHUNG (khÃ´ng cáº§n phÃ¢n tÃ­ch theo tá»«ng bÃ i). "
        "TrÃ¬nh bÃ y theo nÃªu trong system prompt."
    )

    return "\n".join(lines)


def _call_gemini(system_prompt: str, user_prompt: str) -> str:
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=500, detail="Thiáº¿u GEMINI_API_KEY trong mÃ´i trÆ°á»ng.")

    genai.configure(api_key=GEMINI_API_KEY)
    model_name = GEMINI_MODEL
    model = genai.GenerativeModel(model_name=model_name, system_instruction=system_prompt)
    resp = model.generate_content(user_prompt)
    text = getattr(resp, "text", None)
    if not text:
        try:
            text = resp.candidates[0].content.parts[0].text
        except Exception:
            text = ""
    if not text:
        raise HTTPException(status_code=502, detail="Gemini khÃ´ng tráº£ vá» ná»™i dung há»£p lá»‡.")
    return text

def analyze_news(payload: NewsAnalysisInput) -> NewsAnalysisResponse:
    user_prompt = _build_user_prompt(payload)
    analysis = _call_gemini(SYSTEM_PROMPT_FOR_BULK_ANALYSIS, user_prompt)
    return NewsAnalysisResponse(analysis=analysis)