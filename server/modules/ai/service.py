from __future__ import annotations
from fastapi import Request
from typing import List, Dict, Any
from pathlib import Path
from textwrap import dedent
from joblib import load
import numpy as np
import json
from server.modules.ai.schemas import MultipleNewsInput, ClassificationMultipleNewsOutput, ClassificationNewOutput, NewsAnalysisResponse, NewsInput
from server.config import MODEL_PATH
import text_hammer as th


# ===================== Preprocessing =====================
def text_preprocessing(text: str) -> str:
    text = (text or "").lower()
    text = th.cont_exp(text)                # don't -> do not
    text = th.remove_rt(text)               # remove "rt"
    text = th.remove_emails(text)           # user@example.com
    text = th.remove_urls(text)             # http, https
    text = th.remove_html_tags(text)        # <p>...</p>
    text = th.remove_stopwords(text)        # i, me, my, ...
    text = th.remove_accented_chars(text)   # caf√© -> cafe
    text = th.remove_special_chars(text)    # #, @, ...
    text = th.make_base(text)               # ran -> run
    return text.strip()


# ===================== Model (cached) =====================
_MODEL = None
_CLASSES = None  # will hold np.ndarray like [0,1,2]

def _get_model():
    global _MODEL, _CLASSES
    if _MODEL is None:
        path = Path(MODEL_PATH)
        if not path.exists():
            raise FileNotFoundError(f"MODEL_PATH not found: {path}")
        _MODEL = load(path)
        if not hasattr(_MODEL, "predict_proba"):
            raise AttributeError("Loaded model does not implement predict_proba")
        if not hasattr(_MODEL, "classes_"):
            raise AttributeError("Loaded model has no attribute classes_")
        _CLASSES = np.array(_MODEL.classes_)  # expect [0,1,2]
    return _MODEL


# ===================== Inference helpers =====================
def predict_sentiment(pipe, text: str, preprocess_fn=None) -> Dict[str, Any]:
    """Tr·∫£ v·ªÅ processed_text, proba theo l·ªõp g·ªëc, v√† nh√£n d·ª± ƒëo√°n."""
    processed_text = preprocess_fn(text) if preprocess_fn else text
    proba = pipe.predict_proba([processed_text])[0]  # shape (n_classes,)
    classes = getattr(pipe, "classes_", range(len(proba)))
    proba_dict = {int(k): float(v) for k, v in zip(classes, proba)}
    label = pipe.predict([processed_text])[0]
    return {
        "processed_text": processed_text,
        "proba": proba_dict,
        "label": label,
    }


def _map_012_to_pos_neg_neu(proba_by_class: Dict[int, float]) -> Dict[str, float]:

    neg = float(proba_by_class.get(0, 0.0))
    neu = float(proba_by_class.get(1, 0.0))
    pos = float(proba_by_class.get(2, 0.0))

    # ƒë·∫£m b·∫£o t·ªïng ~ 1.0 (trong tr∆∞·ªùng h·ª£p model/proba rounding)
    s = neg + neu + pos
    if s > 0:
        neg, neu, pos = neg / s, neu / s, pos / s

    return {"pos": pos, "neg": neg, "neu": neu}


# ===================== Main service =====================
def classify_news(news_data: List[NewsInput]) -> ClassificationMultipleNewsOutput:
    model = _get_model()
    results: List[ClassificationNewOutput] = []

    for news in news_data:
        title = news.title or ""
        description = news.description or ""
        publish_date = news.publish_date

        text = f"{title} {description}".strip()
        pred = predict_sentiment(model, text, preprocess_fn=text_preprocessing)
        mapped = _map_012_to_pos_neg_neu(pred["proba"])

        results.append(
            ClassificationNewOutput(
                title=title,
                description=description,
                publish_date=publish_date,
                **mapped,
            )
        )

    return ClassificationMultipleNewsOutput(news=results)


# server/services/ai_service.py
import os
from typing import List
from fastapi import HTTPException, Request
from dotenv import load_dotenv
from server.modules.ai.schemas import NewsAnalysisResponse, NewsAnalysisInput
import google.generativeai as genai
from openai import OpenAI
load_dotenv()



OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL   = os.getenv("OPENAI_MODEL", "gpt-5-nano")

SYSTEM_PROMPT_FOR_BULK_ANALYSIS = dedent("""
[ROLE / SYSTEM]  
B·∫°n l√† m·ªôt nh√† ƒë·∫ßu t∆∞ v√† chuy√™n gia ph√¢n t√≠ch th·ªã tr∆∞·ªùng t√†i ch√≠nh.  
Nhi·ªám v·ª• c·ªßa b·∫°n: ƒë·ªçc danh s√°ch c√°c tin t·ª©c (m·ªói tin c√≥ title v√† description, c√≥ th·ªÉ l√† ti·∫øng Anh) v√† ph√¢n lo·∫°i g√≥c nh√¨n t√¢m l√Ω c·ªßa nh√† giao d·ªãch theo 3 nh√≥m:  
- Positive (tin t·ª©c mang l·∫°i k·ª≥ v·ªçng, ni·ªÅm tin, ƒë·ªông l·ª±c ƒë·∫ßu t∆∞ ho·∫∑c t√°c ƒë·ªông t√≠ch c·ª±c ƒë·∫øn th·ªã tr∆∞·ªùng/t√†i s·∫£n).  
- Neutral (tin t·ª©c trung l·∫≠p, ch·ªâ cung c·∫•p th√¥ng tin, ch∆∞a ƒë·ªß ƒë·ªÉ t√°c ƒë·ªông m·∫°nh t·ªõi t√¢m l√Ω th·ªã tr∆∞·ªùng).  
- Negative (tin t·ª©c mang l·∫°i lo ng·∫°i, r·ªßi ro, t√¢m l√Ω bi quan ho·∫∑c t√°c ƒë·ªông ti√™u c·ª±c ƒë·∫øn th·ªã tr∆∞·ªùng/t√†i s·∫£n).  

‚ö†Ô∏è QUAN TR·ªåNG: Lu√¥n d·ªãch v√† vi·∫øt ph·∫ßn ph·∫£n h·ªìi **b·∫±ng ti·∫øng Vi·ªát** d√π tin t·ª©c g·ªëc l√† ti·∫øng Anh.  

[OUTPUT FORMAT]  
Vi·∫øt theo d·∫°ng tin nh·∫Øn, tr·∫£ l·ªùi ti·∫øng Vi·ªát, c√≥ emoji v√† xu·ªëng d√≤ng r√µ r√†ng, v√≠ d·ª•: 
---
üìä Ph√¢n t√≠ch tin t·ª©c {ten_ng·ªØ_c·∫£nh}:  

‚úÖ **Positive:** 
- [Ti√™u ƒë·ªÅ] - [M√¥ t·∫£] - (T·ª´ {th·ªùi_gian_ƒëƒÉng_b√†i})
‚öñÔ∏è **Neutral:** 
- ...
‚ö†Ô∏è **Negative:** 
- ...
üìå **K·∫øt lu·∫≠n:** [K·∫øt lu·∫≠n ng·∫Øn g·ªçn t√¢m l√Ω chung ]
---
[NOTE]  
- C√≥ th·ªÉ r√∫t g·ªçn m√¥ t·∫£ ƒë·ªÉ gi·ªëng tin nh·∫Øn Zalo.  
- T·∫•t c·∫£ ƒë·∫ßu ra b·∫Øt bu·ªôc l√† ti·∫øng Vi·ªát.  
- Ph·∫ßn **K·∫øt lu·∫≠n** ph·∫£i kh√°ch quan, t·ªïng h·ª£p t·ª´ c√°c nh√≥m tin, kh√¥ng th√™m quan ƒëi·ªÉm c√° nh√¢n.
""").strip()


def _build_user_prompt(payload):
    lines: List[str] = []
    lines.append("D·ªØ li·ªáu ƒë·∫ßu v√†o g·ªìm nhi·ªÅu b√†i:")
    for i, item in enumerate(payload.news):
        lines.append(f"\n--- B√†i #{i} ---")
        lines.append(f"Title: {item.title}")
        lines.append(f"Description: {item.description}")
        if getattr(item, "publish_date", None):
            lines.append(f"Publish Date: {item.publish_date}")
        lines.append(f"Scores: pos={item.pos:.3f}, neg={item.neg:.3f}, neu={item.neu:.3f}")
    lines.append(
        "\nY√™u c·∫ßu: Ch·ªâ tr·∫£ v·ªÅ PH√ÇN T√çCH CHUNG (kh√¥ng c·∫ßn ph√¢n t√≠ch theo t·ª´ng b√†i). "
        "Tr√¨nh b√†y theo n√™u trong system prompt."
    )
    return "\n".join(lines)


def _call_chatgpt(system_prompt: str, user_prompt: str) -> str:
    if not OPENAI_API_KEY:
        raise HTTPException(status_code=500, detail="Thi·∫øu OPENAI_API_KEY trong m√¥i tr∆∞·ªùng.")
    
    client = OpenAI(api_key=OPENAI_API_KEY)
    resp = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )
    
    text = resp.choices[0].message.content.strip()
    if not text:
        raise HTTPException(status_code=502, detail="ChatGPT kh√¥ng tr·∫£ v·ªÅ n·ªôi dung h·ª£p l·ªá.")
    return text


def analyze_news(payload):
    user_prompt = _build_user_prompt(payload)
    analysis = _call_chatgpt(SYSTEM_PROMPT_FOR_BULK_ANALYSIS, user_prompt)
    return {"analysis": analysis}


async def get_chat_history(
    request: Request,
    session_id: str,
    limit: int = 100,
    offset: int = 0,
):
    pool = request.app.state.pool

    sql = """
        SELECT 
            id,
            session_id,
            message
        FROM n8n_chat_histories
        WHERE session_id = $1
        ORDER BY id ASC
        LIMIT $2 OFFSET $3
    """

    async with pool.acquire() as conn:
        rows = await conn.fetch(sql, session_id, limit, offset)

    items = []
    for r in rows:
        msg = r["message"]
        # N·∫øu message b·ªã l∆∞u d∆∞·ªõi d·∫°ng string JSON, parse l·∫°i
        if isinstance(msg, str):
            try:
                msg = json.loads(msg)
            except json.JSONDecodeError:
                pass

        items.append({
            "id": r["id"],
            "session_id": r["session_id"],
            "message": msg,
        })

    return {
        "session_id": session_id,
        "items": items,
        "page": {"limit": limit, "offset": offset, "total": len(items)},
    }